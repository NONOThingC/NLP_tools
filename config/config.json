 {
    "disable_tqdm":True,
    "exp_name": "webnlg_star",#"nyt","nyt_star",webnlg,webnlg_star
    "rel2id": "rel2id.json",
    "device_num": 0,
    # "encoder": "BiLSTM",
    "encoder": "BERT",
    "hyper_parameters": {
        "shaking_type": "cln_plus",
        # cat, cat_plus, cln, cln_plus; Experiments show that cat/cat_plus work better with BiLSTM, while cln/cln_plus work better with BERT. The results in the paper are produced by "cat". So, if you want to reproduce the results, "cat" is enough, no matter for BERT or BiLSTM.
        "inner_enc_type": "lstm",#not change here
        # valid only if cat_plus or cln_plus is set. It is the way how to encode inner tokens between each token pairs. If you only want to reproduce the results, just leave it alone.
        "dist_emb_size": -1,
        # -1: do not use distance embedding; other number: need to be larger than the max_seq_len of the inputs. set -1 if you only want to reproduce the results in the paper.
        "ent_add_dist": False,
        # set true if you want add distance embeddings for each token pairs. (for entity decoder)
        "rel_add_dist": False,  # the same as above (for relation decoder)
        "match_pattern": "only_head_text",
        # only_head_text (nyt_star, webnlg_star), whole_text (nyt, webnlg), only_head_index, whole_span
    },
    "training_method":"self-training",#"self-training","self-emsembling","increment-training"
    "use_two_model": False,
    "two_models_hyper_parameters":{
        "student_dropout": 0.4,
        "student_decay": 0,  # 0 equal to not decay
    },
    "use_strategy":False,
     "strategy_hyper_parameters":{
         "Z_RATIO":0.3,
     },# use this when use_strategy==False
    #"strategy_hyper_parameters":{
    #        "enh_rate":2,
    #        "relh_rate":2,
    #    },# use this when use_strategy==True
    "LABEL_OF_TRAIN": 0.3,
}